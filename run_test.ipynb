{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "  sys.path.append(module_path)\n",
    "\n",
    "%env OPENAI_API_KEY = \"*\"\n",
    "from PIL import Image\n",
    "from IPython.core.display import HTML\n",
    "from functools import partial\n",
    "\n",
    "from engine.utils import ProgramGenerator, ProgramInterpreter\n",
    "from prompts.gqa_agent import *\n",
    "from engine.step_interpreters import parse_step\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = partial(create_prompt_stepname_selective)\n",
    "generator = ProgramGenerator(prompter=prompter)\n",
    "interpreter = ProgramInterpreter(dataset='gqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_case(image_path, question):\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail((640,640),Image.Resampling.LANCZOS)\n",
    "    state = dict(\n",
    "        IMAGE=image.convert('RGB')\n",
    "    )\n",
    "    caption, _, _ = interpreter.execute('X=CAP(image=IMAGE)',state,inspect=True)\n",
    "    state['context'] = f'multimodal input: IMAGE ({image.size[0]}wx{image.size[1]}h)\\n\\nCAPTION_IMAGE=\"{caption}\"\\n\\nQ=\"{question}\"\\n\\n<End of Context>\\n\\nThought(Analyze the question: '\n",
    "    shots = 0\n",
    "    success = False\n",
    "    direct_VQA_result, prog_state, html_str = interpreter.execute(f'X=VQA(image=IMAGE,question=\"{question}\")',state,inspect=True)\n",
    "    display(HTML(html_str))\n",
    "    while shots < 15:\n",
    "        response, _ = generator.generate(dict(input=state['context']))\n",
    "        state['context'] += response + '))'\n",
    "        try:\n",
    "            statement = response.split('Implement(')[1] + ')'\n",
    "            result, prog_state, html_str = interpreter.execute(statement,state,inspect=True)\n",
    "            display(HTML(html_str))\n",
    "            step_name = parse_step(statement)['step_name']\n",
    "            if 'RESULT' in step_name:\n",
    "              return result, state['context']\n",
    "            elif 'CROP' not in step_name:\n",
    "              state['context'] += '\\n<result>\\n' + str(result) + '\\n</result>\\nThought(Analyze the result: '\n",
    "            else:\n",
    "              output_var = parse_step(statement)['output_var']\n",
    "              caption, _, _ = interpreter.execute(f'X=CAP(image={output_var})',state,inspect=True)\n",
    "              state['context'] += '\\n<result>\\n' + caption + '\\n</result>\\nThought(Analyze the result: '\n",
    "        except:\n",
    "          response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                  {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Read the following problem analysis procedure. It is incomplete, and you need to deduce the most likely answer. If I ask the VQA system directly about the IMAGE, it will respond with \\\"{direct_VQA_result}.\\\" Based on all the available information, **PROVIDE YOUR BEST GUESS**. Note that the accuracy of VQA is not 100% so you should have an analysis based on both the following procedure and the VQA result.\\n{state['context']}\"\n",
    "                  }\n",
    "                ]\n",
    "              )\n",
    "          response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                  {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Read the following analysis, DIRECTLY give me the answer to the question WITHOUT any analysis.\\n{response['choices'][0]['message']['content']}\"\n",
    "                  }\n",
    "                ]\n",
    "              )\n",
    "          return response['choices'][0]['message']['content'], state['context']\n",
    "        shots += 1\n",
    "    response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                  {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Read the following problem analysis procedure. It is incomplete, and you need to deduce the most likely answer. If I ask the VQA system directly about the IMAGE, it will respond with \\\"{direct_VQA_result}.\\\" Based on all the available information, **PROVIDE YOUR BEST GUESS**. Note that the accuracy of VQA is not 100% so you should have an analysis based on both the following procedure and the VQA result.\\n{state['context']}\"\n",
    "                  }\n",
    "                ]\n",
    "              )\n",
    "    response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"Read the following analysis, DIRECTLY give me the answer to the question WITHOUT any analysis.\\n{response['choices'][0]['message']['content']}\"\n",
    "            }\n",
    "          ]\n",
    "        )\n",
    "    return response['choices'][0]['message']['content'], state['context']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
