import os
# Enforce the download/cache directory for Hugging Face models.
os.environ["HF_HOME"] = "/home/hice1/yxu846/scratch/models"
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from .step_interpreters import register_step_interpreters, parse_step


class Program:
    def __init__(self, prog_str, init_state=None):
        self.prog_str = prog_str
        self.state = init_state if init_state is not None else {}
        self.instructions = self.prog_str.split('\n')


class ProgramInterpreter:
    def __init__(self, dataset='nlvr'):
        self.step_interpreters = register_step_interpreters(dataset)

    def execute_step(self, prog_step, inspect):
        step_name = parse_step(prog_step.prog_str, partial=True)['step_name']
        print(step_name)
        return self.step_interpreters[step_name].execute(prog_step, inspect)

    def execute(self, prog, init_state, inspect=False):
        results = []
        if isinstance(prog, str):
            prog = Program(prog, init_state)
        else:
            assert isinstance(prog, Program)

        prog_steps = [Program(instruction, init_state=prog.state)
                      for instruction in prog.instructions]

        html_str = '<hr>'
        for prog_step in prog_steps:
            if inspect:
                step_output, step_html = self.execute_step(prog_step, inspect)
                html_str += step_html + '<hr>'
            else:
                step_output = self.execute_step(prog_step, inspect)
            results.append(step_output)

        if inspect:
            return step_output, prog.state, html_str

        return step_output, prog.state


class ProgramGenerator:
    def __init__(self, prompter, model_name_or_path="THUDM/glm-4-9b-hf",
                 temperature=0.7, top_p=0.5, prob_agg='mean'):
        """
        Initialize the ProgramGenerator using Hugging Face Transformers to load
        the Meta-Llama-3.1-70B model with offloading to avoid GPU OOM issues.

        Args:
            prompter: A function that takes inputs and returns a prompt string.
            model_name_or_path: The Hugging Face model identifier (or local path) to load.
            temperature: Generation temperature.
            top_p: Top-p (nucleus) sampling parameter.
            prob_agg: Aggregation method for probabilities (not used here).
        """
        self.prompter = prompter
        self.temperature = temperature
        self.top_p = top_p
        self.prob_agg = prob_agg

        # Load the tokenizer and model from the given path,
        # enforcing the cache directory to be /home/hice1/yxu846/scratch/models.
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            cache_dir="/home/hice1/yxu846/scratch/models"
        )

        # Use max_memory and offload_folder to avoid OOM when loading the huge 70B model.
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            cache_dir="/home/hice1/yxu846/scratch/models",
            device_map="auto",            # Automatically map layers to available devices.
            torch_dtype=torch.float16,      # Use fp16 precision.
            low_cpu_mem_usage=True
        )

        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer
        )

    def compute_prob(self, response):
        """
        The Hugging Face text-generation pipeline does not provide token-level log probabilities.
        Here, we return a default probability value.
        """
        return 1.0

    def generate(self, inputs):
        """
        Generate a program based on the input prompt.
        """
        prompt_text = self.prompter(inputs)
        input_ids = self.tokenizer(prompt_text)["input_ids"]
        generated = self.generator(
            prompt_text,
            max_length=len(input_ids) + 256,  # Safeguard maximum generation length.
            temperature=self.temperature,
            top_p=self.top_p,
            do_sample=True,
            early_stopping=True,
            eos_token_id=self.tokenizer.eos_token_id,  # Let the model stop when it predicts EOS.
            pad_token_id=self.tokenizer.eos_token_id,
        )
        # The pipeline returns a list of dictionaries with a "generated_text" key.
        if generated and "generated_text" in generated[0]:
            full_text = generated[0]["generated_text"].strip()
            # Extract only the generated continuation beyond the prompt.
            prog = full_text[len(prompt_text):].strip().split('\n\n')[0].strip()
        else:
            prog = ""
        prob = self.compute_prob(generated)
        return prog, prob, prompt_text
